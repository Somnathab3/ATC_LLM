#!/usr/bin/env python3
"""
Real LLM Communication Verification Test

This script verifies that the LLM communication is REAL and NOT mocked by:
1. Testing direct Ollama connection
2. Sending real prompts to LLM and verifying responses
3. Testing SCAT -> BlueSky -> LLM -> BlueSky communication chain
4. Providing extensive debugging output

All mock modes are forcibly disabled to ensure real LLM communication.
"""

import json
import logging
import os
import sys
import time
import traceback
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add source directory to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Configure comprehensive logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('real_llm_verification.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class RealLLMVerifier:
    """Comprehensive real LLM communication verification."""
    
    def __init__(self):
        self.ollama_host = "http://127.0.0.1:11434"
        self.model_name = "llama3.1:8b"
        self.timeout = 60
        self.debug_interactions = []
        
        # Force disable all mock modes
        self._disable_all_mocks()
        
        logger.info("[INIT] Real LLM Verifier initialized - NO MOCKING ALLOWED")
    
    def _disable_all_mocks(self):
        """Forcibly disable all mock modes."""
        os.environ.pop("LLM_DISABLED", None)
        os.environ.pop("LLM_MOCK", None)
        os.environ["LLM_REAL_MODE"] = "1"
        os.environ["LLM_STRICT_VERIFY"] = "1"
        logger.info("[OK] All mock modes forcibly disabled")
    
    def test_ollama_connection(self) -> bool:
        """Test basic Ollama server connection."""
        logger.info("[CONNECT] Testing Ollama server connection...")
        
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            
            if response.status_code == 200:
                models = response.json()
                model_names = [m.get('name', '') for m in models.get('models', [])]
                logger.info(f"[OK] Ollama connected. Available models: {model_names}")
                
                if self.model_name in model_names:
                    logger.info(f"[OK] Target model '{self.model_name}' is available")
                    return True
                else:
                    logger.error(f"[ERROR] Target model '{self.model_name}' not found")
                    return False
            else:
                logger.error(f"[ERROR] Ollama server error: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"[ERROR] Connection failed: {e}")
            return False
    
    def call_ollama_api(self, prompt: str) -> Dict[str, Any]:
        """Direct Ollama API call with error handling."""
        try:
            logger.info(f"[SEND] Sending prompt to LLM (length: {len(prompt)} chars)")
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.2,
                        "top_p": 0.9
                    }
                },
                timeout=self.timeout
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "")
                
                logger.info(f"üì• LLM response received ({response_time:.2f}s)")
                logger.debug(f"Response preview: {response_text[:200]}...")
                
                # Log interaction
                interaction = {
                    "timestamp": datetime.now().isoformat(),
                    "prompt_length": len(prompt),
                    "response_length": len(response_text),
                    "response_time_sec": response_time,
                    "status": "success"
                }
                self.debug_interactions.append(interaction)
                
                return {"success": True, "response": response_text, "time": response_time}
            else:
                logger.error(f"[ERROR] API error: {response.status_code}")
                return {"success": False, "error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            logger.error(f"[ERROR] API call failed: {e}")
            return {"success": False, "error": str(e)}
    
    def test_basic_llm_response(self) -> bool:
        """Test basic LLM response to verify it's real."""
        logger.info("[TEST] Testing basic LLM response...")
        
        prompt = """You are an expert air traffic controller. Please respond with EXACTLY this JSON format:

{
    "status": "operational",
    "current_time": "REPLACE_WITH_ACTUAL_TIME",
    "verification_code": "REAL_LLM_123",
    "test_calculation": "What is 17 * 23?",
    "response_uniqueness": "Generate a random 6-digit number"
}

Replace the placeholder values with actual responses. This verifies you are a real LLM."""

        result = self.call_ollama_api(prompt)
        
        if not result["success"]:
            logger.error("[ERROR] Basic LLM test failed")
            return False
        
        try:
            # Try to extract JSON from response
            response_text = result["response"]
            
            # Find JSON in response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(0))
                
                # Verify expected fields
                required_fields = ["status", "verification_code", "test_calculation"]
                if all(field in json_data for field in required_fields):
                    logger.info(f"[OK] Real LLM verified: {json_data}")
                    return True
                else:
                    logger.error(f"[ERROR] Response missing required fields: {json_data}")
                    return False
            else:
                logger.error("[ERROR] No JSON found in LLM response")
                logger.debug(f"Raw response: {response_text}")
                return False
                
        except json.JSONDecodeError as e:
            logger.error(f"[ERROR] JSON parsing failed: {e}")
            return False
    
    def test_aviation_prompt(self) -> bool:
        """Test aviation-specific prompt similar to the system."""
        logger.info("‚úàÔ∏è Testing aviation-specific prompt...")
        
        prompt = """You are an expert Air Traffic Controller with ICAO certification.

TASK: Analyze the following aircraft situation for potential conflicts.

AIRCRAFT DATA:
- Ownship: SAS101, A320, Position: 59.6519degN 17.9186degE, Altitude: 35000 ft, Speed: 450 kt, Heading: 090deg
- Traffic: DLH456, B737, Position: 59.6600degN 17.9500degE, Altitude: 35000 ft, Speed: 430 kt, Heading: 270deg

STANDARDS:
- Minimum horizontal separation: 5.0 nautical miles
- Minimum vertical separation: 1000 feet
- Lookahead time: 10 minutes

Respond with JSON only:
{
    "conflict_detected": true/false,
    "closest_approach_distance_nm": <number>,
    "time_to_closest_approach_min": <number>,
    "requires_resolution": true/false,
    "assessment": "detailed explanation"
}"""

        result = self.call_ollama_api(prompt)
        
        if not result["success"]:
            logger.error("[ERROR] Aviation prompt test failed")
            return False
        
        try:
            response_text = result["response"]
            
            # Extract JSON
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(0))
                
                # Verify aviation-specific fields
                required_fields = ["conflict_detected", "closest_approach_distance_nm", "assessment"]
                if all(field in json_data for field in required_fields):
                    logger.info(f"[OK] Aviation analysis successful: {json_data}")
                    return True
                else:
                    logger.error(f"[ERROR] Missing aviation fields: {json_data}")
                    return False
            else:
                logger.error("[ERROR] No JSON in aviation response")
                logger.debug(f"Raw response: {response_text}")
                return False
                
        except json.JSONDecodeError as e:
            logger.error(f"[ERROR] Aviation JSON parsing failed: {e}")
            return False
    
    def test_resolution_prompt(self) -> bool:
        """Test conflict resolution prompt."""
        logger.info("[PROCESS] Testing conflict resolution prompt...")
        
        prompt = """You are an expert Air Traffic Controller providing conflict resolution.

SITUATION:
- Aircraft: SAS101 (A320)
- Conflict detected with DLH456 at 35000 ft
- Current heading: 090deg, Speed: 450 kt
- Time to conflict: 5 minutes

RESOLUTION TASK:
Provide a heading change to resolve the conflict.

Respond with JSON only:
{
    "action": "HEADING_CHANGE",
    "new_heading_deg": <number 0-359>,
    "turn_direction": "left|right",
    "bluesky_command": "SAS101 HDG xxx",
    "rationale": "explanation of resolution strategy"
}"""

        result = self.call_ollama_api(prompt)
        
        if not result["success"]:
            logger.error("[ERROR] Resolution prompt test failed")
            return False
        
        try:
            response_text = result["response"]
            
            # Extract JSON
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(0))
                
                # Verify resolution fields
                required_fields = ["action", "new_heading_deg", "bluesky_command"]
                if all(field in json_data for field in required_fields):
                    
                    # Verify BlueSky command format
                    bluesky_cmd = json_data.get("bluesky_command", "")
                    if "SAS101 HDG" in bluesky_cmd:
                        logger.info(f"[OK] Resolution successful: {json_data}")
                        return True
                    else:
                        logger.error(f"[ERROR] Invalid BlueSky command: {bluesky_cmd}")
                        return False
                else:
                    logger.error(f"[ERROR] Missing resolution fields: {json_data}")
                    return False
            else:
                logger.error("[ERROR] No JSON in resolution response")
                logger.debug(f"Raw response: {response_text}")
                return False
                
        except json.JSONDecodeError as e:
            logger.error(f"[ERROR] Resolution JSON parsing failed: {e}")
            return False
    
    def load_scat_sample(self, scat_file: str) -> Dict[str, Any]:
        """Load sample SCAT data for testing."""
        logger.info(f"[LOAD] Loading SCAT sample: {scat_file}")
        
        try:
            with open(scat_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract basic flight info
            fpl_base = data.get('fpl', {}).get('fpl_base', [])
            plots = data.get('plots', [])
            
            if not fpl_base or not plots:
                logger.error("[ERROR] Invalid SCAT data structure")
                return {}
            
            aircraft_info = fpl_base[0]
            latest_plot = plots[-1] if plots else {}
            
            # Convert to simple format
            aircraft_state = {
                "callsign": aircraft_info.get("callsign", "UNKNOWN"),
                "aircraft_type": aircraft_info.get("aircraft_type", "UNKNOWN"),
                "position": latest_plot.get("I062/105", {}),
                "altitude": latest_plot.get("I062/136", {}),
                "velocity": latest_plot.get("I062/185", {}),
                "timestamp": latest_plot.get("time_of_track", "")
            }
            
            logger.info(f"[OK] SCAT data loaded: {aircraft_state['callsign']}")
            return aircraft_state
            
        except Exception as e:
            logger.error(f"[ERROR] SCAT loading failed: {e}")
            return {}
    
    def test_scat_to_llm_chain(self, scat_file: str) -> bool:
        """Test complete SCAT -> LLM -> BlueSky chain."""
        logger.info("[CONVERT] Testing complete SCAT -> LLM -> BlueSky chain...")
        
        # Step 1: Load SCAT data
        aircraft_data = self.load_scat_sample(scat_file)
        if not aircraft_data:
            return False
        
        # Step 2: Create LLM prompt with SCAT data
        position = aircraft_data.get("position", {})
        altitude_data = aircraft_data.get("altitude", {})
        velocity = aircraft_data.get("velocity", {})
        
        lat = position.get("lat", 0)
        lon = position.get("lon", 0)
        alt = altitude_data.get("measured_flight_level", 0) * 100  # Convert FL to feet
        vx = velocity.get("vx", 0)
        vy = velocity.get("vy", 0)
        
        prompt = f"""You are an expert Air Traffic Controller analyzing real SCAT surveillance data.

AIRCRAFT STATE FROM SCAT DATA:
- Callsign: {aircraft_data['callsign']}
- Aircraft Type: {aircraft_data['aircraft_type']}
- Position: {lat:.4f}degN {lon:.4f}degE
- Altitude: {alt:.0f} feet
- Velocity: Vx={vx} kt, Vy={vy} kt
- Timestamp: {aircraft_data['timestamp']}

TASK: 
1. Analyze this aircraft state
2. Generate a BlueSky command to change heading to 270deg
3. Provide safety assessment

Respond with JSON only:
{{
    "scat_analysis": "analysis of SCAT data quality and aircraft state",
    "bluesky_command": "{aircraft_data['callsign']} HDG 270",
    "safety_assessment": "safety evaluation of the command",
    "data_source": "SCAT",
    "timestamp": "current_time"
}}"""

        # Step 3: Send to LLM
        result = self.call_ollama_api(prompt)
        
        if not result["success"]:
            logger.error("[ERROR] SCAT -> LLM chain failed")
            return False
        
        # Step 4: Parse response and verify BlueSky command
        try:
            response_text = result["response"]
            
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(0))
                
                bluesky_cmd = json_data.get("bluesky_command", "")
                expected_pattern = f"{aircraft_data['callsign']} HDG 270"
                
                if expected_pattern in bluesky_cmd:
                    logger.info(f"[OK] Complete chain successful!")
                    logger.info(f"   SCAT data: {aircraft_data['callsign']} at {lat:.4f}, {lon:.4f}")
                    logger.info(f"   LLM analysis: {json_data.get('scat_analysis', 'N/A')}")
                    logger.info(f"   BlueSky command: {bluesky_cmd}")
                    return True
                else:
                    logger.error(f"[ERROR] Incorrect BlueSky command: {bluesky_cmd}")
                    return False
            else:
                logger.error("[ERROR] No JSON in chain response")
                return False
                
        except json.JSONDecodeError as e:
            logger.error(f"[ERROR] Chain JSON parsing failed: {e}")
            return False
    
    def run_iterative_test(self, max_iterations: int = 3) -> bool:
        """Test iterative BlueSky <-> LLM communication."""
        logger.info(f"[CONVERT] Testing iterative communication ({max_iterations} rounds)...")
        
        # Initial aircraft state
        aircraft = {
            "callsign": "TEST123",
            "position": {"lat": 59.5, "lon": 18.0},
            "altitude": 35000,
            "heading": 90,
            "speed": 450
        }
        
        iteration_results = []
        
        for i in range(max_iterations):
            logger.info(f"\n[CONVERT] === ITERATION {i+1}/{max_iterations} ===")
            
            # Create situation prompt
            prompt = f"""Air Traffic Controller iteration {i+1}.

CURRENT AIRCRAFT STATE:
- Callsign: {aircraft['callsign']}
- Position: {aircraft['position']['lat']:.4f}degN {aircraft['position']['lon']:.4f}degE
- Altitude: {aircraft['altitude']} ft
- Heading: {aircraft['heading']}deg
- Speed: {aircraft['speed']} kt

TASK: Make a small course adjustment and provide the BlueSky command.

Respond with JSON:
{{
    "iteration": {i+1},
    "new_heading": <number 0-359>,
    "bluesky_command": "{aircraft['callsign']} HDG <new_heading>",
    "reason": "brief explanation"
}}"""

            result = self.call_ollama_api(prompt)
            
            if not result["success"]:
                logger.error(f"[ERROR] Iteration {i+1} failed")
                break
            
            try:
                import re
                json_match = re.search(r'\{.*\}', result["response"], re.DOTALL)
                if json_match:
                    json_data = json.loads(json_match.group(0))
                    
                    new_heading = json_data.get("new_heading", aircraft["heading"])
                    bluesky_cmd = json_data.get("bluesky_command", "")
                    
                    # Update aircraft state
                    aircraft["heading"] = new_heading
                    
                    iteration_results.append({
                        "iteration": i+1,
                        "command": bluesky_cmd,
                        "new_heading": new_heading,
                        "success": True
                    })
                    
                    logger.info(f"[OK] Iteration {i+1}: {bluesky_cmd}")
                else:
                    logger.error(f"[ERROR] Iteration {i+1}: No JSON response")
                    break
                    
            except json.JSONDecodeError:
                logger.error(f"[ERROR] Iteration {i+1}: JSON parse error")
                break
        
        success_rate = len(iteration_results) / max_iterations
        logger.info(f"[FINISH] Iterative test: {len(iteration_results)}/{max_iterations} successful ({success_rate:.1%})")
        
        return success_rate >= 0.8  # 80% success rate required
    
    def run_full_verification(self, scat_file: str = None) -> Dict[str, Any]:
        """Run complete verification suite."""
        logger.info("[INIT] Starting COMPLETE Real LLM Verification")
        logger.info("=" * 60)
        
        start_time = time.time()
        test_results = {}
        
        # Test 1: Ollama connection
        test_results["ollama_connection"] = self.test_ollama_connection()
        
        # Test 2: Basic LLM response
        test_results["basic_llm_response"] = self.test_basic_llm_response()
        
        # Test 3: Aviation prompt
        test_results["aviation_prompt"] = self.test_aviation_prompt()
        
        # Test 4: Resolution prompt
        test_results["resolution_prompt"] = self.test_resolution_prompt()
        
        # Test 5: SCAT chain (if file provided)
        if scat_file and os.path.exists(scat_file):
            test_results["scat_chain"] = self.test_scat_to_llm_chain(scat_file)
        else:
            test_results["scat_chain"] = None
            logger.warning("[WARN] SCAT file not provided or not found - skipping SCAT chain test")
        
        # Test 6: Iterative communication
        test_results["iterative_communication"] = self.run_iterative_test()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Compile results
        results = {
            "verification_timestamp": datetime.now().isoformat(),
            "total_time_sec": total_time,
            "test_results": test_results,
            "debug_interactions": self.debug_interactions,
            "summary": {
                "total_tests": len([k for k, v in test_results.items() if v is not None]),
                "passed_tests": len([k for k, v in test_results.items() if v is True]),
                "failed_tests": len([k for k, v in test_results.items() if v is False]),
                "overall_success": all(v for v in test_results.values() if v is not None)
            }
        }
        
        return results
    
    def save_results(self, results: Dict[str, Any]):
        """Save verification results to file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"real_llm_verification_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"[SAVE] Results saved to: {filename}")
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to save results: {e}")


def main():
    """Main execution function."""
    print("[INIT] Real LLM Communication Verification")
    print("=" * 50)
    print("This tool verifies that LLM communication is REAL and NOT mocked.")
    print()
    
    # Check for SCAT file
    scat_file = None
    if len(sys.argv) > 1:
        scat_file = sys.argv[1]
        if os.path.exists(scat_file):
            print(f"[LOAD] SCAT file: {scat_file}")
        else:
            print(f"[WARN] SCAT file not found: {scat_file}")
            scat_file = None
    else:
        # Try default SCAT file
        default_scat = "scenarios/scat/100000.json"
        if os.path.exists(default_scat):
            scat_file = default_scat
            print(f"[LOAD] Using default SCAT file: {scat_file}")
        else:
            print("[INFO] No SCAT file provided (SCAT chain test will be skipped)")
    
    print()
    
    # Create verifier and run tests
    verifier = RealLLMVerifier()
    
    try:
        results = verifier.run_full_verification(scat_file)
        
        # Save results
        verifier.save_results(results)
        
        # Print summary
        print("\n" + "=" * 50)
        print("[STATS] VERIFICATION RESULTS")
        print("=" * 50)
        
        test_results = results["test_results"]
        summary = results["summary"]
        
        for test_name, result in test_results.items():
            if result is True:
                print(f"[OK] {test_name.replace('_', ' ').title()}: PASSED")
            elif result is False:
                print(f"[ERROR] {test_name.replace('_', ' ').title()}: FAILED")
            else:
                print(f"[WARN] {test_name.replace('_', ' ').title()}: SKIPPED")
        
        print()
        print(f"[STATS] Summary: {summary['passed_tests']}/{summary['total_tests']} tests passed")
        print(f"[TIME] Total time: {results['total_time_sec']:.2f} seconds")
        print(f"[CONVERT] LLM interactions: {len(results['debug_interactions'])}")
        
        if summary["overall_success"]:
            print("\n[SUCCESS] OVERALL RESULT: SUCCESS - Real LLM communication verified!")
            print("[OK] All communication paths working:")
            print("  * Direct LLM communication")
            print("  * Aviation-specific prompts")
            print("  * Conflict resolution")
            if test_results.get("scat_chain"):
                print("  * SCAT -> LLM -> BlueSky chain")
            print("  * Iterative communication loops")
        else:
            print("\n[ERROR] OVERALL RESULT: FAILED - Issues detected")
            print("Please check the debug log for details.")
        
        print(f"\n[SAVE] Detailed results: real_llm_verification_*.json")
        print(f"[SAVE] Debug log: real_llm_verification.log")
        
    except KeyboardInterrupt:
        print("\n[WARN] Verification interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n[ERROR] Verification failed: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
